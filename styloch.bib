
@inproceedings{k_argasinski_stylometric_2024,
	address = {Cham},
	title = {Stylometric {Analysis} of {Large} {Language} {Model}-{Generated} {Commentaries} in the {Context} of {Medical} {Neuroscience}},
	copyright = {All rights reserved},
	isbn = {978-3-031-63775-9},
	doi = {10.1007/978-3-031-63775-9_20},
	abstract = {This study investigates the application of Large Language Models (LLMs) in generating commentaries on neuroscientific papers, with a focus on their stylometric differences from human-written texts. Utilizing three papers from reputable journals in the field of medical neuroscience, each accompanied by published expert commentaries, we compare these with commentaries generated by state-of-the-art LLMs. Through quantitative stylometric analysis and qualitative assessments, we aim to be a part of the discussion around the viability of LLMs in augmenting scientific discourse within the domain of medical neuroscience.},
	language = {en},
	booktitle = {Computational {Science} – {ICCS} 2024},
	publisher = {Springer Nature Switzerland},
	author = {K. Argasiński, Jan and Grabska-Gradzińska, Iwona and Przystalski, Karol and K. Ochab, Jeremi and Walkowiak, Tomasz},
	editor = {Franco, Leonardo and de Mulatier, Clélia and Paszynski, Maciej and Krzhizhanovskaya, Valeria V. and Dongarra, Jack J. and Sloot, Peter M. A.},
	year = {2024},
	pages = {281--295},
	file = {K. Argasiński et al2024-Computational Science – ICCS 2024-Stylometric.pdf:D\:\\OneDrive - Uniwersytet Jagielloński\\Zotero\\JKOchab\\K. Argasiński et al2024-Computational Science – ICCS 2024-Stylometric.pdf:application/pdf},
}

@inproceedings{karajgikar_implementing_2024,
	address = {Arlington, VA},
	title = {Implementing interpretable models in stylometric analysis},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://dh24-abstracts.netlify.app/assets/ochab_jeremi_k__implementing_interpretable_models_in_stylome},
	doi = {10.5281/ZENODO.13761079},
	abstract = {The Book of Abstracts for the Alliance of Digital Humanities Organizations (ADHO) annual conference in Arlington, VA, hosted by George Mason University.},
	language = {en},
	urldate = {2024-09-17},
	booktitle = {{DH2024} {Book} of {Abstracts}},
	publisher = {George Mason University},
	author = {Ochab, Jeremi K and Walkowiak, Tomasz},
	editor = {Karajgikar, Jajwalya and Janco, Andrew and Otis, Jessica},
	month = sep,
	year = {2024},
	pages = {515--517},
}

@article{przystalski_stylometry_2026,
	title = {Stylometry recognizes human and {LLM}-generated texts in short samples},
	volume = {296},
	copyright = {All rights reserved},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417425026181},
	doi = {10.1016/j.eswa.2025.129001},
	abstract = {The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans, addressing issues of model attribution, intellectual property, and ethical AI use. Stylometry has been used extensively to characterise the style and attribute authorship of texts. By applying it to LLM-generated texts, we identify their emergent writing patterns. The paper involves creating a benchmark dataset based on Wikipedia, with (a) human-written term summaries, (b) texts generated purely by LLMs (GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods (Dipper, T5). The 10-sentence long texts were classified by tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based (our own pipeline) stylometric features that encode lexical, grammatical, syntactic, and punctuation patterns. The cross-validated results reached a performance of up to 0.87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between 0.79 and 1. in binary classification, with the particular example of Wikipedia and GPT-4 reaching up to 0.98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed features characteristic of the encyclopaedic text type, individual overused words, as well as a greater grammatical standardisation of LLMs with respect to human-written texts. These results show – crucially, in the context of the increasingly sophisticated LLMs – that it is possible to distinguish machine- from human-generated texts at least for a well-defined text type},
	urldate = {2025-07-24},
	journal = {Expert Systems with Applications},
	author = {Przystalski, Karol and Argasiński, Jan K. and Grabska-Gradzińska, Iwona and Ochab, Jeremi K.},
	month = jan,
	year = {2026},
	keywords = {AI detection, benchmark dataset, Benchmark dataset, large language models, Large language models, machine-generated text detection, Machine-generated text detection, stylometry, Stylometry},
	pages = {129001},
	file = {ScienceDirect Snapshot:C\:\\Users\\JOchab\\Zotero\\storage\\M9DUBB23\\S0957417425026181.html:text/html},
}

@inproceedings{ochab_styloch_2025,
	series = {{CEUR} {Workshop} {Proceedings}},
	title = {{StylOch} at {PAN}: {Gradient}-{Boosted} {Trees} with {Frequency}-{Based} {Stylometric} {Features}},
	volume = {XXXX},
	url = {https://ceur-ws.org/Vol-XXXX/paper-XXX.pdf},
	booktitle = {Working {Notes} of the {Conference} and {Labs} of the {Evaluation} {Forum} ({CLEF} 2025), {Madrid}, {Spain}, 9-12 {September}, 2025},
	publisher = {CEUR-WS.org},
	author = {Ochab, Jeremi K. and Matias, Mateusz and Boba, Tymoteusz and Walkowiak, Tomasz},
	editor = {Faggioli, Guglielmo and Ferro, Nicola and Rosso, Paolo and Spina, Damiano},
	year = {2025},
	keywords = {⛔ No DOI found},
	pages = {3836--3845},
}
